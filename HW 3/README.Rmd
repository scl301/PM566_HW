---
title: "Assignment 3: Web Scraping and Text Mining"
author: "Stephanie Lee"
date: "`r Sys.Date()`"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(httr)
library(xml2)
library(stringr)
library(tidyverse)
library(tidytext)
library(dplyr)
library(ggplot2)
library(forcats)
library(data.table)
```

## APIs

### Number of Papers
```{r num papers}
# Downloading the website
website <- xml2::read_html("https://pubmed.ncbi.nlm.nih.gov/?term=sars-cov-2+trial+vaccine")

# Finding the counts
counts <- xml2::xml_find_first(website, "/html/body/main/div[9]/div[2]/div[2]/div[1]")

# Turning it into text
counts <- as.character(counts)

# Extracting the data using regex
stringr::str_extract(counts, "[0-9,]+")
```

### IDs

```{r query}
query_ids <- GET(
  url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi",
  query = list(db= "pubmed", term= "sars-cov-2 trial vaccine", retmax= 250)
)

ids <- httr::content(query_ids)
```

```{r article details}
ids <- as.character(ids)
ids <- stringr::str_extract_all(ids, "<Id>[[:digit:]]+</Id>")[[1]]
ids <- stringr::str_remove_all(ids, "</?Id>")
```


```{r publications}
publications <- GET(
  url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi",
  query = list(
    db      = "pubmed",
    id      = paste(ids, collapse = ","),
    rettype = "abstract"
    )
)
publications <- httr::content(publications)
publications_txt <- as.character(publications)
```

```{r dataset}

# Details
pub_char_list <- xml2::xml_children(publications)
pub_char_list <- sapply(pub_char_list, as.character)

# Titles
titles <- str_extract(pub_char_list, "<ArticleTitle>(\\n|.)+</ArticleTitle>")
titles <- str_remove_all(titles, "</?[[:alnum:]]+>")
titles <- str_replace_all(titles, "\\s+"," ")

# Abstracts
abstracts <- str_extract(pub_char_list, "<Abstract>[[:print:][:space:]]+</Abstract>")
abstracts <- str_remove_all(abstracts, "</?[[:alnum:]- =\"]+>")
abstracts <- str_replace_all(abstracts, "[[:space:]]+", " ")

# Journals
journals <- str_extract(pub_char_list, "<Title>(\\n|.)+</Title>")
journals<- str_remove_all(journals, "</?[[:alnum:]]+>")
journals<- str_replace_all(journals, "\\s+"," ")

# Dates
dates <- str_remove_all(pub_char_list, "\\n") %>%
    str_extract("<PubDate>.*</PubDate>")
dates <-str_remove_all(dates, "</?[[:alnum:]]+>")
##remove new line and spaces
dates <- str_replace_all(dates, "\\s+"," ")

# Create Database
database <- data.frame(
  PubMedID = ids,
  Title    = titles,
  Journal = journals,
  Abstract = abstracts,
  Date = dates
)
knitr::kable(database[1:2,], caption = "Sars-cov-2 Trial Vaccine Publications")
```


## Text Mining

```{r pubmed data}
if (!file.exists("pubmed.csv")){
  download.file("https://raw.githubusercontent.com/USCbiostats/data-science-data/master/03_pubmed/pubmed.csv", "pubmed.csv", method="libcurl", timeout = 60)
}
pubmed <- read.csv("pubmed.csv")
pubmed <- as_tibble(pubmed)
```

**1. Tokenize the abstracts and count the number of each token. Do you see anything interesting? Does removing stop words change what tokens appear as the most frequent? What are the 5 most common tokens for each search term after removing stopwords?**

```{r token counts}
pubmed %>%
  unnest_tokens(token, abstract) %>%
  count(token, sort = TRUE) # %>%
  #top_n(20, n) %>%
  #ggplot(aes(n, fct_reorder(token, n))) +
  #geom_col()
```

The most frequently occurring tokens are stop words.

```{r stopword removal}
pubmed %>%
  unnest_tokens(word, abstract) %>%
  count(word, sort = TRUE) %>%
  anti_join(stop_words, by = c("word")) %>%
  top_n(5, n)
```
After the stop words are removed, there is definitely a change in the most frequent tokens. The top 5 tokens are now "covid", "19", "patients", "cancer", and "prostate".


**2. Tokenize the abstracts into bigrams. Find the 10 most common bigram and visualize them with ggplot2.**

```{r top 10 bigrams}
pubmed %>%
  unnest_ngrams(bigram, abstract, n=2) %>%
  separate(bigram, into = c("first", "second"), sep = " ",remove = FALSE) %>%
  anti_join(stop_words, by = c(first = "word")) %>%
  anti_join(stop_words, by = c(second = "word")) %>%
  count(bigram, sort = TRUE) %>%
  top_n(10, n) %>%
  ggplot(aes(n, fct_reorder(bigram, n))) +
  geom_col()+
  labs(y = "bigram", title = "Top 10 Most Frequent Bigrams (Without Stop Words)") 
```

**3. Calculate the TF-IDF value for each word-search term combination. (here you want the search term to be the “document”) What are the 5 tokens from each search term with the highest TF-IDF value? How are the results different from the answers you got in question 1?**

```{r tf-idf}

term_table <-pubmed %>%
  group_by(term) %>% 
  unnest_tokens(word, abstract) %>%
  count(word, sort = TRUE) %>%
    bind_tf_idf(word, term, n)

term_table %>%
    top_n(5,tf_idf) %>% 
    arrange(desc(term)) %>%
knitr::kable(digits =4, align=c("l", "c", "c", "c","c","c"), caption = "Top 5 TF-IDFs by Search Term")

```
Factoring the TF-IDF values provides greater information on the relevancy of a term. These terms differ from those in question 1, as they are more specific to terminology and conditions rather than simply the frequency of all words independently.